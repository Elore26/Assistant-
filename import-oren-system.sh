#!/bin/bash
#############################################################
# Script d'import automatique - Oren System depuis Supabase
#
# Ce script rÃ©cupÃ¨re TOUT depuis ton projet Supabase :
#   - SchÃ©ma SQL complet (tables, fonctions, triggers, RLS)
#   - DonnÃ©es de toutes les tables
#   - Fichiers du Storage (tous les buckets)
#   - Edge Functions
#
# Usage: ./import-oren-system.sh
#############################################################

set -e

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SUPABASE_URL="https://jmfbhsbkjrizrcovgkqs.supabase.co"
SUPABASE_ANON_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImptZmJoc2JranJpenJjb3Zna3FzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzA0NTM5NjksImV4cCI6MjA4NjAyOTk2OX0.NA3WRZVCrj7cANLV94RQC6T8qPVuq0kiJAbEoLV2pxY"
PROJECT_REF="jmfbhsbkjrizrcovgkqs"
REPO_DIR="$(cd "$(dirname "$0")" && pwd)"
BRANCH="claude/import-oren-system-files-zncPS"

# Couleurs
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m'

log()   { echo -e "${GREEN}[âœ“]${NC} $1"; }
warn()  { echo -e "${YELLOW}[!]${NC} $1"; }
err()   { echo -e "${RED}[âœ—]${NC} $1"; }
info()  { echo -e "${BLUE}[i]${NC} $1"; }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VÃ‰RIFICATION DES PRÃ‰REQUIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "   Import Oren System depuis Supabase"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Demander la service_role key si pas dÃ©jÃ  dÃ©finie
if [ -z "$SUPABASE_SERVICE_ROLE_KEY" ]; then
    echo -e "${YELLOW}Pour un export complet, la service_role key est recommandÃ©e.${NC}"
    echo "Tu la trouves dans: Supabase Dashboard > Settings > API > service_role"
    echo ""
    read -rp "Colle ta service_role key (ou appuie sur EntrÃ©e pour utiliser la clÃ© anon) : " SERVICE_KEY
    if [ -n "$SERVICE_KEY" ]; then
        SUPABASE_SERVICE_ROLE_KEY="$SERVICE_KEY"
    else
        SUPABASE_SERVICE_ROLE_KEY="$SUPABASE_ANON_KEY"
        warn "Utilisation de la clÃ© anon - certaines tables protÃ©gÃ©es par RLS pourraient ne pas Ãªtre exportÃ©es"
    fi
fi

API_KEY="$SUPABASE_SERVICE_ROLE_KEY"

# VÃ©rifier les outils nÃ©cessaires
for cmd in curl jq git; do
    if ! command -v "$cmd" &>/dev/null; then
        err "$cmd n'est pas installÃ©. Installe-le et relance le script."
        exit 1
    fi
done
log "PrÃ©requis OK (curl, jq, git)"

# Test de connexion
info "Test de connexion Ã  Supabase..."
HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
    -H "apikey: $API_KEY" \
    -H "Authorization: Bearer $API_KEY" \
    "$SUPABASE_URL/rest/v1/")

if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
    log "Connexion Ã  Supabase OK"
else
    err "Impossible de se connecter Ã  Supabase (HTTP $HTTP_CODE)"
    err "VÃ©rifie ton URL et ta clÃ© API"
    exit 1
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CRÃ‰ATION DE LA STRUCTURE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
info "CrÃ©ation de la structure de dossiers..."
mkdir -p "$REPO_DIR/supabase/migrations"
mkdir -p "$REPO_DIR/supabase/seed"
mkdir -p "$REPO_DIR/supabase/functions"
mkdir -p "$REPO_DIR/supabase/storage"
log "Structure crÃ©Ã©e"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. EXPORT DU SCHÃ‰MA SQL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• Export du schÃ©ma SQL â•â•â•"

# RÃ©cupÃ©rer la liste des tables via l'OpenAPI spec
info "RÃ©cupÃ©ration de la liste des tables..."
OPENAPI=$(curl -s \
    -H "apikey: $API_KEY" \
    -H "Authorization: Bearer $API_KEY" \
    "$SUPABASE_URL/rest/v1/" \
    -H "Accept: application/openapi+json")

if echo "$OPENAPI" | jq . &>/dev/null 2>&1; then
    # Extract table names from OpenAPI paths
    TABLES=$(echo "$OPENAPI" | jq -r '.paths | keys[] | ltrimstr("/")' 2>/dev/null | grep -v '^$' | sort)

    if [ -z "$TABLES" ]; then
        # Alternative: extract from definitions
        TABLES=$(echo "$OPENAPI" | jq -r '.definitions | keys[]' 2>/dev/null | grep -v '^$' | sort)
    fi

    if [ -n "$TABLES" ]; then
        TABLE_COUNT=$(echo "$TABLES" | wc -l)
        log "TrouvÃ© $TABLE_COUNT table(s)"
        echo "$TABLES" | while read -r table; do
            echo "    - $table"
        done
    else
        warn "Aucune table trouvÃ©e via OpenAPI"
    fi

    # Sauvegarder le spec OpenAPI complet
    echo "$OPENAPI" | jq '.' > "$REPO_DIR/supabase/openapi-spec.json" 2>/dev/null
    log "Spec OpenAPI sauvegardÃ©e dans supabase/openapi-spec.json"

    # GÃ©nÃ©rer un fichier SQL de schÃ©ma Ã  partir de l'OpenAPI
    info "GÃ©nÃ©ration du schÃ©ma SQL Ã  partir de l'OpenAPI spec..."

    SCHEMA_FILE="$REPO_DIR/supabase/migrations/001_schema.sql"
    echo "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" > "$SCHEMA_FILE"
    echo "-- Oren System - SchÃ©ma de base de donnÃ©es" >> "$SCHEMA_FILE"
    echo "-- ExportÃ© depuis Supabase le $(date '+%Y-%m-%d %H:%M:%S')" >> "$SCHEMA_FILE"
    echo "-- Projet: $PROJECT_REF" >> "$SCHEMA_FILE"
    echo "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" >> "$SCHEMA_FILE"
    echo "" >> "$SCHEMA_FILE"

    # Generate CREATE TABLE statements from OpenAPI definitions
    echo "$OPENAPI" | python3 -c "
import json, sys

try:
    spec = json.load(sys.stdin)
except:
    sys.exit(0)

definitions = spec.get('definitions', {})

type_map = {
    'integer': 'INTEGER',
    'number': 'NUMERIC',
    'string': 'TEXT',
    'boolean': 'BOOLEAN',
    'object': 'JSONB',
    'array': 'JSONB',
}

format_map = {
    'timestamp with time zone': 'TIMESTAMPTZ',
    'timestamp without time zone': 'TIMESTAMP',
    'uuid': 'UUID',
    'bigint': 'BIGINT',
    'smallint': 'SMALLINT',
    'real': 'REAL',
    'double precision': 'DOUBLE PRECISION',
    'json': 'JSON',
    'jsonb': 'JSONB',
    'text': 'TEXT',
    'integer': 'INTEGER',
    'date': 'DATE',
    'time with time zone': 'TIMETZ',
    'time without time zone': 'TIME',
    'interval': 'INTERVAL',
    'bytea': 'BYTEA',
    'inet': 'INET',
    'cidr': 'CIDR',
    'macaddr': 'MACADDR',
    'numeric': 'NUMERIC',
}

for table_name, table_def in sorted(definitions.items()):
    if table_name.startswith('_'):
        continue
    props = table_def.get('properties', {})
    required = table_def.get('required', [])

    if not props:
        continue

    print(f'CREATE TABLE IF NOT EXISTS public.\"{table_name}\" (')
    columns = []
    for col_name, col_def in props.items():
        col_format = col_def.get('format', '')
        col_type = col_def.get('type', 'text')
        col_desc = col_def.get('description', '')

        sql_type = format_map.get(col_format, type_map.get(col_type, 'TEXT'))

        # Check for primary key hint
        pk = ''
        if col_desc and 'primary key' in col_desc.lower():
            pk = ' PRIMARY KEY'

        # Check for default
        default = ''
        if col_def.get('default') is not None:
            default_val = col_def['default']
            if isinstance(default_val, str):
                default = f\" DEFAULT '{default_val}'\"
            elif isinstance(default_val, bool):
                default = f\" DEFAULT {'true' if default_val else 'false'}\"
            else:
                default = f' DEFAULT {default_val}'

        nullable = '' if col_name in required else ''

        columns.append(f'    \"{col_name}\" {sql_type}{pk}{default}{nullable}')

    print(',\n'.join(columns))
    print(');')
    print()
" >> "$SCHEMA_FILE" 2>/dev/null

    log "SchÃ©ma SQL gÃ©nÃ©rÃ© dans supabase/migrations/001_schema.sql"
else
    warn "Impossible de parser l'OpenAPI spec"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. EXPORT DES DONNÃ‰ES DE CHAQUE TABLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• Export des donnÃ©es â•â•â•"

if [ -n "$TABLES" ]; then
    echo "$TABLES" | while read -r table; do
        [ -z "$table" ] && continue
        info "Export de la table: $table"

        DATA=$(curl -s \
            -H "apikey: $API_KEY" \
            -H "Authorization: Bearer $API_KEY" \
            -H "Accept: application/json" \
            -H "Prefer: count=exact" \
            "$SUPABASE_URL/rest/v1/$table?select=*&limit=10000")

        if echo "$DATA" | jq . &>/dev/null 2>&1; then
            ROW_COUNT=$(echo "$DATA" | jq 'length')
            echo "$DATA" | jq '.' > "$REPO_DIR/supabase/seed/${table}.json"
            log "  $table: $ROW_COUNT lignes exportÃ©es"

            # Aussi gÃ©nÃ©rer un fichier SQL d'insertion
            if [ "$ROW_COUNT" -gt 0 ]; then
                echo "$DATA" | python3 -c "
import json, sys
table = '$table'
data = json.load(sys.stdin)
if not data:
    sys.exit(0)

print(f'-- Seed data for {table}')
print(f'-- {len(data)} rows')
print()

for row in data:
    cols = ', '.join(f'\"{k}\"' for k in row.keys())
    vals = []
    for v in row.values():
        if v is None:
            vals.append('NULL')
        elif isinstance(v, bool):
            vals.append('TRUE' if v else 'FALSE')
        elif isinstance(v, (int, float)):
            vals.append(str(v))
        elif isinstance(v, (dict, list)):
            vals.append(\"'\" + json.dumps(v).replace(\"'\", \"''\") + \"'::jsonb\")
        else:
            vals.append(\"'\" + str(v).replace(\"'\", \"''\") + \"'\")

    val_str = ', '.join(vals)
    print(f'INSERT INTO public.\"{table}\" ({cols}) VALUES ({val_str});')

print()
" > "$REPO_DIR/supabase/seed/${table}.sql" 2>/dev/null
            fi
        else
            warn "  $table: impossible d'exporter (RLS bloquant?)"
            echo "$DATA" > "$REPO_DIR/supabase/seed/${table}.error.log"
        fi
    done
else
    warn "Aucune table Ã  exporter"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. EXPORT DU STORAGE (BUCKETS ET FICHIERS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• Export du Storage â•â•â•"

BUCKETS=$(curl -s \
    -H "apikey: $API_KEY" \
    -H "Authorization: Bearer $API_KEY" \
    "$SUPABASE_URL/storage/v1/bucket")

if echo "$BUCKETS" | jq . &>/dev/null 2>&1; then
    BUCKET_COUNT=$(echo "$BUCKETS" | jq 'length')

    if [ "$BUCKET_COUNT" -gt 0 ]; then
        log "TrouvÃ© $BUCKET_COUNT bucket(s)"
        echo "$BUCKETS" | jq '.' > "$REPO_DIR/supabase/storage/buckets.json"

        echo "$BUCKETS" | jq -r '.[].name' | while read -r bucket; do
            [ -z "$bucket" ] && continue
            info "Exploration du bucket: $bucket"
            mkdir -p "$REPO_DIR/supabase/storage/$bucket"

            # Lister les fichiers du bucket
            FILES=$(curl -s \
                -H "apikey: $API_KEY" \
                -H "Authorization: Bearer $API_KEY" \
                -X POST \
                -H "Content-Type: application/json" \
                -d '{"prefix":"","limit":1000,"offset":0,"sortBy":{"column":"name","order":"asc"}}' \
                "$SUPABASE_URL/storage/v1/object/list/$bucket")

            if echo "$FILES" | jq . &>/dev/null 2>&1; then
                FILE_COUNT=$(echo "$FILES" | jq 'length')
                log "  $bucket: $FILE_COUNT fichier(s)/dossier(s)"
                echo "$FILES" | jq '.' > "$REPO_DIR/supabase/storage/$bucket/_manifest.json"

                # TÃ©lÃ©charger chaque fichier
                echo "$FILES" | jq -r '.[] | select(.id != null) | .name' | while read -r filename; do
                    [ -z "$filename" ] && continue
                    info "  TÃ©lÃ©chargement: $bucket/$filename"
                    curl -s \
                        -H "apikey: $API_KEY" \
                        -H "Authorization: Bearer $API_KEY" \
                        -o "$REPO_DIR/supabase/storage/$bucket/$filename" \
                        "$SUPABASE_URL/storage/v1/object/$bucket/$filename"
                done

                # Explorer les sous-dossiers
                echo "$FILES" | jq -r '.[] | select(.id == null) | .name' | while read -r folder; do
                    [ -z "$folder" ] && continue
                    info "  Exploration du sous-dossier: $bucket/$folder"
                    mkdir -p "$REPO_DIR/supabase/storage/$bucket/$folder"

                    SUBFILES=$(curl -s \
                        -H "apikey: $API_KEY" \
                        -H "Authorization: Bearer $API_KEY" \
                        -X POST \
                        -H "Content-Type: application/json" \
                        -d "{\"prefix\":\"$folder\",\"limit\":1000,\"offset\":0,\"sortBy\":{\"column\":\"name\",\"order\":\"asc\"}}" \
                        "$SUPABASE_URL/storage/v1/object/list/$bucket")

                    echo "$SUBFILES" | jq -r '.[] | select(.id != null) | .name' 2>/dev/null | while read -r subfile; do
                        [ -z "$subfile" ] && continue
                        info "    TÃ©lÃ©chargement: $bucket/$folder/$subfile"
                        curl -s \
                            -H "apikey: $API_KEY" \
                            -H "Authorization: Bearer $API_KEY" \
                            -o "$REPO_DIR/supabase/storage/$bucket/$folder/$subfile" \
                            "$SUPABASE_URL/storage/v1/object/$bucket/$folder/$subfile"
                    done
                done
            else
                warn "  $bucket: impossible de lister les fichiers"
            fi
        done
    else
        warn "Aucun bucket trouvÃ©"
    fi
else
    warn "Impossible de lister les buckets (vÃ©rifier les permissions)"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. EXPORT DES EDGE FUNCTIONS (via Supabase CLI si dispo)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• Edge Functions â•â•â•"

if command -v supabase &>/dev/null; then
    info "Supabase CLI dÃ©tectÃ©, tentative de tÃ©lÃ©chargement des fonctions..."
    cd "$REPO_DIR"
    supabase functions download --project-ref "$PROJECT_REF" 2>&1 || warn "Impossible de tÃ©lÃ©charger les Edge Functions via CLI"
else
    warn "Supabase CLI non installÃ© - les Edge Functions ne peuvent pas Ãªtre exportÃ©es automatiquement"
    info "Pour les exporter manuellement:"
    info "  1. npm install -g supabase"
    info "  2. supabase login"
    info "  3. supabase functions download --project-ref $PROJECT_REF"

    # Essayer quand mÃªme de lister les fonctions via l'API
    info "Tentative de listage des fonctions via l'API Management..."

    # Note: cette API nÃ©cessite un access token (pas la clÃ© anon)
    mkdir -p "$REPO_DIR/supabase/functions"
    cat > "$REPO_DIR/supabase/functions/README.md" << 'FUNCEOF'
# Edge Functions

Les Edge Functions n'ont pas pu Ãªtre exportÃ©es automatiquement.

## Pour les exporter manuellement :

```bash
npm install -g supabase
supabase login
supabase link --project-ref jmfbhsbkjrizrcovgkqs
supabase functions download --project-ref jmfbhsbkjrizrcovgkqs
```
FUNCEOF
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. EXPORT DES RPC FUNCTIONS (fonctions PostgreSQL)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• RPC Functions â•â•â•"

# Essayer d'appeler les fonctions RPC connues
RPC_FUNCS=$(echo "$OPENAPI" | jq -r '.paths | to_entries[] | select(.value.post.tags[]? == "rpc" or .key | startswith("/rpc/")) | .key | ltrimstr("/rpc/") | ltrimstr("/")' 2>/dev/null)

if [ -n "$RPC_FUNCS" ]; then
    RPC_COUNT=$(echo "$RPC_FUNCS" | wc -l)
    log "TrouvÃ© $RPC_COUNT fonction(s) RPC"
    echo "$RPC_FUNCS" > "$REPO_DIR/supabase/migrations/rpc_functions.txt"
    echo "$RPC_FUNCS" | while read -r func; do
        echo "    - $func"
    done
else
    info "Aucune fonction RPC trouvÃ©e dans l'OpenAPI spec"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. GÃ‰NÃ‰RATION DU CONFIG SUPABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• GÃ©nÃ©ration de la configuration â•â•â•"

cat > "$REPO_DIR/supabase/config.toml" << TOMLEOF
# Oren System - Configuration Supabase
# GÃ©nÃ©rÃ© automatiquement le $(date '+%Y-%m-%d %H:%M:%S')

[project]
id = "$PROJECT_REF"

[api]
enabled = true
port = 54321
schemas = ["public", "storage", "graphql_public"]
extra_search_path = ["public", "extensions"]
max_rows = 1000

[db]
port = 54322
major_version = 15

[studio]
enabled = true
port = 54323

[auth]
enabled = true
site_url = "http://localhost:3000"

[storage]
enabled = true
file_size_limit = "50MiB"
TOMLEOF

log "Configuration gÃ©nÃ©rÃ©e dans supabase/config.toml"

# GÃ©nÃ©rer .env.example
cat > "$REPO_DIR/.env.example" << ENVEOF
# Oren System - Variables d'environnement
SUPABASE_URL=$SUPABASE_URL
SUPABASE_ANON_KEY=$SUPABASE_ANON_KEY
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key_here
ENVEOF

log ".env.example gÃ©nÃ©rÃ©"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. GIT COMMIT & PUSH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
info "â•â•â• Git commit & push â•â•â•"

cd "$REPO_DIR"

# S'assurer qu'on est sur la bonne branche
CURRENT_BRANCH=$(git branch --show-current)
if [ "$CURRENT_BRANCH" != "$BRANCH" ]; then
    git checkout -b "$BRANCH" 2>/dev/null || git checkout "$BRANCH"
fi

# Ajouter les fichiers
git add supabase/ .env.example

# VÃ©rifier qu'il y a des changements
if git diff --cached --quiet; then
    warn "Aucun changement Ã  committer"
else
    git commit -m "Import Oren System files from Supabase

- Database schema (tables, types, constraints)
- Seed data (JSON + SQL format)
- Storage buckets and files
- Supabase configuration
- Environment variables template"

    # Push
    git push -u origin "$BRANCH" && log "Push rÃ©ussi!" || warn "Push Ã©chouÃ© - tu devras push manuellement"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ‰SUMÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${GREEN}   Import terminÃ© !${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Structure crÃ©Ã©e :"
find "$REPO_DIR/supabase" -type f | sort | while read -r f; do
    echo "  ğŸ“„ ${f#$REPO_DIR/}"
done
echo ""
echo "  ğŸ“„ .env.example"
echo ""
log "Tout est prÃªt !"
